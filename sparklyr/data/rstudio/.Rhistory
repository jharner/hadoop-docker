knitr::opts_chunk$set(echo = TRUE)
if(system("hostname", intern=TRUE) == "gben3001.hpc.wvu.edu"){
hdfs.path <- "/user/eharner";
spark.master <- "yarn-client"
} else {
hdfs.path <- "/user/rstudio";
spark.master <- "local"}
options(warn=-1)
library(rhdfs)
hdfs.init()
hdfs.ls(path=hdfs.path)
# Using sapply:
small.ints = 1:100
sapply(small.ints, function(x) x^2)
library(rmr2)
# Using MapReduce: note that rmr2 has its own interface with hdfs
small.ints = to.dfs(1:100)
# Now define the mapreduce function
out.data <- mapreduce(
input = small.ints,
map = function(k, v) cbind(v, v^2))
out.data <- from.dfs(out.data)
str(out.data)
head(out.data$val)
library(SparkR)
sc <- sparkR.init(master = spark.master)
# If you run Spark 2.0, you do not need to create a Spark Context. You can simpley create a SparkR session.
# sparkR.session(sparkConfig = list(spark.driver.memory = '4g'))
sqlContext <- sparkRSQL.init(sc)
# In Spark 2.0 you do not need to create a SQLContext
library(magrittr)
faithful.df <- createDataFrame(sqlContext, faithful)
head(faithful.df)
# In Spark 2.0 simply use:
# faithful.df <- as.DataFrame(faithful)
# Select the eruptions column using pipes
head(faithful.df %>% select(faithful.df$eruptions))
# Or without pipes:
# head(select(faithful.df, faithful.df$eruptions))
# Select rows with waiting times less than 50 using pipes
head(faithful.df %>% filter(faithful.df$waiting < 50))
# Or without pipes:
# head(filter(faithful.df, faithful.df$waiting < 50))
sparkR.stop()
library(sparklyr)
sc <- spark_connect(master = spark.master)
clear
clear()
knitr::opts_chunk$set(echo = TRUE)
if(system("hostname", intern=TRUE) == "gben3001.hpc.wvu.edu"){
hdfs.path <- "/user/eharner";
spark.master <- "yarn-client"
} else {
hdfs.path <- "/user/rstudio";
spark.master <- "local"}
library(sparklyr)
sc <- spark_connect(master = "local")
knitr::opts_chunk$set(echo = TRUE)
if(system("hostname", intern=TRUE) == "gben3001.hpc.wvu.edu"){
hdfs.path <- "/user/eharner";
spark.master <- "yarn-client"
} else {
hdfs.path <- "/user/rstudio";
spark.master <- "local"}
options(warn=-1)
library(rhdfs)
hdfs.init()
hdfs.ls(path=hdfs.path)
# Using sapply:
small.ints = 1:100
sapply(small.ints, function(x) x^2)
library(rmr2)
# Using MapReduce: note that rmr2 has its own interface with hdfs
small.ints = to.dfs(1:100)
# Now define the mapreduce function
out.data <- mapreduce(
input = small.ints,
map = function(k, v) cbind(v, v^2))
out.data <- from.dfs(out.data)
str(out.data)
head(out.data$val)
library(SparkR)
sc <- sparkR.init(master = spark.master)
# If you run Spark 2.0, you do not need to create a Spark Context. You can simpley create a SparkR session.
# sparkR.session(sparkConfig = list(spark.driver.memory = '4g'))
sqlContext <- sparkR.init(sc)
# In Spark 2.0 you do not need to create a SQLContext
library(magrittr)
faithful.df <- createDataFrame(sqlContext, faithful)
knitr::opts_chunk$set(echo = TRUE)
if(system("hostname", intern=TRUE) == "gben3001.hpc.wvu.edu"){
hdfs.path <- "/user/eharner";
spark.master <- "yarn-client"
} else {
hdfs.path <- "/user/rstudio";
spark.master <- "local"}
options(warn=-1)
library(rhdfs)
hdfs.init()
hdfs.ls(path=hdfs.path)
# Using sapply:
small.ints = 1:100
sapply(small.ints, function(x) x^2)
library(rmr2)
# Using MapReduce: note that rmr2 has its own interface with hdfs
small.ints = to.dfs(1:100)
# Now define the mapreduce function
out.data <- mapreduce(
input = small.ints,
map = function(k, v) cbind(v, v^2))
out.data <- from.dfs(out.data)
str(out.data)
head(out.data$val)
library(SparkR)
sc <- sparkR.init(master = spark.master)
# If you run Spark 2.0, you do not need to create a Spark Context. You can simpley create a SparkR session.
# sparkR.session(sparkConfig = list(spark.driver.memory = '4g'))
sqlContext <- sparkR.init(sc)
# In Spark 2.0 you do not need to create a SQLContext
library(magrittr)
faithful.df <- createDataFrame(sqlContext, faithful)
knitr::opts_chunk$set(echo = TRUE)
if(system("hostname", intern=TRUE) == "gben3001.hpc.wvu.edu"){
hdfs.path <- "/user/eharner";
spark.master <- "yarn-client"
} else {
hdfs.path <- "/user/rstudio";
spark.master <- "local"}
options(warn=-1)
library(rhdfs)
hdfs.init()
hdfs.ls(path=hdfs.path)
# Using sapply:
small.ints = 1:100
sapply(small.ints, function(x) x^2)
library(rmr2)
# Using MapReduce: note that rmr2 has its own interface with hdfs
small.ints = to.dfs(1:100)
# Now define the mapreduce function
out.data <- mapreduce(
input = small.ints,
map = function(k, v) cbind(v, v^2))
out.data <- from.dfs(out.data)
str(out.data)
head(out.data$val)
library(SparkR)
sc <- sparkR.init(master = spark.master)
# If you run Spark 2.0, you do not need to create a Spark Context. You can simpley create a SparkR session.
# sparkR.session(sparkConfig = list(spark.driver.memory = '4g'))
sqlContext <- sparkRSQL.init(sc)
# In Spark 2.0 you do not need to create a SQLContext
library(magrittr)
faithful.df <- createDataFrame(sqlContext, faithful)
head(faithful.df)
# In Spark 2.0 simply use:
# faithful.df <- as.DataFrame(faithful)
# Select the eruptions column using pipes
head(faithful.df %>% select(faithful.df$eruptions))
# Or without pipes:
# head(select(faithful.df, faithful.df$eruptions))
# Select rows with waiting times less than 50 using pipes
head(faithful.df %>% filter(faithful.df$waiting < 50))
# Or without pipes:
# head(filter(faithful.df, faithful.df$waiting < 50))
sparkR.stop()
library(sparklyr)
sc <- spark_connect(master = spark.master)
